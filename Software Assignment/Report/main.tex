\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}

\geometry{a4paper, margin=1in}

\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\lstset{
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    tabsize=4,
    language=C,
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    captionpos=b
}
\title{Software assignment}
\author{EE24BTECH11050-Pothuri Rahul}
\date{ }

\begin{document}

\maketitle
\section{Eigenvalues}
 Let A be a square matrix of order n and $\lambda$ be a scalar such that, for any non-zero matrix X,
\begin{center}
$AX = \lambda X$
\end{center}
Those values of $\lambda$ are defined as Eigenvalues.\\
Eigenvalues are scalars associated with a linear system of equations, and are used to transform eigenvectors.
\\
Their are many ways to compute the eigenvalues. One of those is by doing QR decomposition. \\
\section{QR Decomposition}
This is the method of factorizing the matrix into two matrices,namely Q and R. Where the matrix Q is orthogonal and matrix R is Upper triangular matrix. \\
Let A be the input matrix. We get Q matrix by Gram-Schmidt orthogonalization process. The Gram-Schmidt orthogonalization process is a method used to convert a set of linearly independent vectors into an orthogonal set. \\
To convert a set of linearly independent vectors into an orthogonal  set using Gram-Schmidt method: \\
Mathematically, we compute the columns of \(Q\) and the entries of \(R\) as:

\begin{itemize}
    \item The \(k\)-th column of \(Q\) is computed as:
    \[
    Q[:,k] = \frac{A[:,k]}{\|A[:,k]\|}
    \]
    where \(\|A[:,k]\|\) is the norm of the \(k\)-th column of \(A\).

    \item The \(R\) matrix entries are computed as:
    \[
    R[k,j] = Q^T A[:,j]
    \]
    for \(j \geq k\).

    \item The matrix \(A\) is then updated by subtracting the projection onto the \(k\)-th column of \(Q\), i.e.,
    \[
    A[:,j] = A[:,j] - Q[:,k] R[k,j]
    \]
    for \(j > k\).
\end{itemize}


\section{How To Find Eigenvalues Using QR Decomposition}
Let $A_0$=A, where A is the input matrix.\\
\begin{center}
    $A_0=Q_0R_0$ \\
   Form $A_1=R_0Q_0$ \\
   then $A_1$ can be factorised into $A_1=Q_1R_1$\\
   Form $A_2=R_1Q_1$ \\
   then $A_2$ can be factorised into $A_2=Q_2R_2$ \\
\end{center}
   Iterate until convergence.That is,till the matrix formed $A_k$ becomes diagonal. \\
   And the diagonal elements in the $A_k$ are the required Eigenvalues of the input matrix A.

\section{\textbf{Explaination Of code}}
\subsection{Libraries used: }
\lstinputlisting[language= C]{library.c}
\textbf{stdio.h} is used for input and output. \\
\textbf{math.h} is used for mathematical operations.
\subsection{Function used for Multiplication of Matrices: }
\lstinputlisting[language= C]{Multiply.c}
\subsection{Function for tarnspose a Matrix: }
\lstinputlisting[language= C]{Trans.c}
\subsection{Function for QR Decomposition: }
\lstinputlisting[language= C]{QR.c}
\begin{enumerate}
    \item Intialise all elements of Q and R to 0.
    \item R[k][k] terms equals to the norm of $k^{th}$ column of A.
    \item The orthogonal vector(Q) is the normalized version of the \( k \)-th column of \( A \), so each entry \( A[i][k] \) is divided by \( R[k][k] \) to ensure that the column in \( Q \) has unit length.
\item For each column \( j \) of \( A \), starting from the \( (k+1) \)-th column, the code computes the projection of the \( j \)-th column of \( A \) onto the \( k \)-th orthogonal vector \( Q[:, k] \). This projection is used to determine the coefficients for the matrix \( R \):
\item Subtracting the Projection from A to Make Columns Orthogonal.
\end{enumerate}
\subsection{Function to find eigenvalues by QR Decomposition}
\lstinputlisting[language= C]{Algo.c}
This algorithm is used to find the eigenvalues by QR decomposition.
The whole process happens here is explained in Section 3.
\subsection{Main function: }
\lstinputlisting[language= C]{main.c}
Main function is used to take the inputs and give out the output.
\subsubsection{Input: }
The order of the Matrix (n) and the elements ($n^2$ elements) are taken as the input of the matrix.
\subsubsection{Output: }
The n-eigenvalues of the input matrix print as output.
\section{Verification}
\subsection{Input: }
5\\
1 2 3 4 5\\
6 7 8 9 10\\
11 12 13 14 15\\
16 17 18 19 20\\
21 22 23 24 25.
\subsection{output}
Eigenvalues of the given Matrix are : \\
68.642082 -3.642080 -0.000000 -0.000000 0.000000 























   
\section{Time Complexity}   
In general, Time complexity of
\begin{enumerate}
\item QR Decomposition is $O(n^3)$
\item Matrix Multiplication is $O(n^3)$
\item Norm calculation and Convergence check  is $O(n^2)$
\end{enumerate}
Hence, Total time complexity is $O(n^3)$ \\
Let m be no.of itrations until convergence, Then Total time complexity is $O(mn^3)$.\\
\section{Comparision Of Algorithms}
As mentioned earlier their are lot many Algorithms to find the eigenvalues of a given matrix.
\begin{enumerate}
\item Power Itration :For m itrations,Time complexity is $O(mn^2)$, but in this method we can compute only the largest eigenvalue among all eigenvalues.Highly accurate for the largest eigenvalue but fails for others. Sensitive to conditioning.
\item Jacobi Method: Time complexity of this method is $O(n^3)$, but this method is used only for the symmetric matrices.Exact for symmetric matrices, numerically stable. Slow convergence for large matrices.
\item Inverse Power Iteration: Requires matrix inversion which is expensive and finds only one eigenvalue when we execute the process. 
\end{enumerate}
Among all these, Finding eigenvalues using QR Decomposition(using Gram-Schmidt ) is better as this algorithm works for any matrix.Time complexity of this process is comparable with all other processes.






\end{document}
